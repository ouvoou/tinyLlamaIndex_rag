import logging
import sys
import torch, os
from llama_index.core import PromptTemplate, Settings, SimpleDirectoryReader, VectorStoreIndex, load_index_from_storage, StorageContext, QueryBundle
from llama_index.core.schema import MetadataMode
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.memory import ChatMemoryBuffer

from llama_index.core.base.llms.types import ChatMessage
from collections import deque

##å®šä¹‰æ—¥å¿—é…ç½®
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

##å®šä¹‰system prompt
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­åŒ»AIåŠ©æ‰‹.è¯·å›ç­”æˆ‘çš„ä¸“ä¸šæ€§é—®é¢˜ã€‚"""
query_wrapper_prompt = PromptTemplate(
    "[INST]<<SYS>>\n" + SYSTEM_PROMPT + "<</SYS>>\n\n{query_str}[/INST] "
)

#è°ƒç”¨æœ¬åœ°LLM
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_path = "/root/autodl-tmp/LlamaIndexrag/LlamaIndex/tcm-ai-rag/tcm-ai-rag/models/Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    local_files_only=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype="auto",
    trust_remote_code=True,
    device_map="auto",
)

Settings.llm = HuggingFaceLLM(
    context_window = 4096,
    max_new_tokens = 512,
    generate_kwargs = {"temperature": 0.0, "do_sample": False},
    query_wrapper_prompt = query_wrapper_prompt,
    tokenizer = tokenizer,
    model = model,
    # model_kwargs = {
    #     "trust_remote_code":True,
    #     "quantization_config": quantization_config,
    #     "device_map": "auto",
    # }
)

Settings.embed_model = HuggingFaceEmbedding(
    model_name="/root/autodl-tmp/LlamaIndexrag/LlamaIndex/tcm-ai-rag/tcm-ai-rag/models/BAAI/bge-base-zh-v1.5"
)

##ä»æœ¬åœ°å‘é‡åº“ä¸­è·å–index
storage_context = StorageContext.from_defaults(persist_dir="/root/autodl-tmp/LlamaIndexrag/LlamaIndex/tcm-ai-rag/tcm-ai-rag/doc_emb")
index = load_index_from_storage(storage_context)

##æ„å»ºæŸ¥è¯¢å¼•æ“
memory = ChatMemoryBuffer.from_defaults(token_limit=2000)
query_engine = index.as_chat_engine(chat_mode="context", memory=memory, streaming=True)


print("\nğŸ’¬ è¾“å…¥ä½ çš„ä¸­åŒ»é—®é¢˜ï¼Œè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºï¼š")

while True:
    user_input = input("ç”¨æˆ·ï¼š")
    if user_input.strip().lower() in ["exit", "quit"]:
        print("ç¨‹åºå·²é€€å‡ºã€‚")
        break
    response = query_engine.chat(user_input)
    print("LLM:", str(response))
